---
layout: single
title: "Paper Reading"
permalink: /reading list/
author_profile: true
---

## AI Chip/Accelerator
* [Nature](https://www.nature.com/search?journal=ncomms,%20nature,%20natelectron&article_type=research&subject=engineering,%20nanoscience-and-technology&order=relevance)
  - Neuro-inspired computing chips(2020)
* [Science](https://www.science.org/action/doSearch?AllField=Edge+learning+using+a+fully+integrated+neuro-inspired+memristor+chip)
  - Edge learning using a fully integrated neuro-inspired memristor chip(2023)
* [ISSCC](https://dblp.org/db/conf/isscc/index.html)
  - A 1.42TOPS/W Deep Convolutional Neural Network Recognition Processor for Intelligent IoE Systems(2016)
  - A 288μW Programmable Deep-Learning Processor with 270KB On-Chip Weight Storage Using Non-Uniform Memory Hierarchy for Mobile Intelligence(2017)
  - A 0.62mW Ultra-Low-Power Convolutional-NeuralNetwork Face-Recognition Processor and a CIS Integrated with Always-On Haar-Like Face Detector(2017)
  - A 2.9TOPS/W Deep Convolutional Neural Network SoC in FD-SOI 28nm for Intelligent Embedded Systems(2017)
  - DNPU: An 8.1TOPS/W Reconfigurable CNN-RNN Processor for General-Purpose Deep Neural Networks(2017)
  - UNPU: A 50.6TOPS/W Unified Deep Neural Network Accelerator with 1b-to-16b Fully-Variable Weight Bit-Precision(2018)
  - A 65nm 4Kb Algorithm-Dependent Computing-inMemory SRAM Unit-Macro with 2.3ns and 55.8TOPS/W Fully Parallel Product-Sum Operation for Binary DNN Edge Processors(2018)
  - A 28nm 64Kb 6T SRAM Computing-in-Memory Macro with 8b MAC Operation for AI Edge Chips(2020)
  - A Programmable Neural-Network Inference Accelerator Based on Scalable In-Memory Computing(2021)
  - An 89TOPS/W and 16.3TOPS/mm2 All-Digital SRAM-Based Full-Precision Compute-In-Memory Macro in 22nm for Machine-Learning Edge Applications(2021)
* [JSSC](https://dblp.org/db/journals/jssc/index.html)
  - Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks(2017)
  - A High Energy Efficient Reconfigurable Hybrid Neural Network Processor for Deep Learning Applications(2018)
  - Evolver: A Deep Learning Processor With On-Device Quantization–Voltage–Frequency Tuning(2021)
  - TranCIM: Full-Digital Bitline-Transpose CIM-based Sparse Transformer Accelerator With Pipeline/Parallel Reconfigurable Modes(2023)
  - ReDCIM: Reconfigurable Digital ComputingIn-Memory Processor With Unified FP/INT Pipeline for Cloud AI Acceleration(2023)
* [VLSI](https://dblp.org/db/conf/vlsic/index.html)
  - A 1.06-to-5.09 TOPS/W reconfigurable hybrid-neural-network processor for deep learning applications(2017)
  - A 40nm Analog-Input ADC-Free Compute-in-Memory RRAM Macro with Pulse-Width Modulation between Sub-arrays(2022)
* [TVLSI](https://dblp.org/db/journals/tvlsi/index.html)
  - Deep Convolutional Neural Network Architecture With Reconfigurable Computation Patterns(2017)
  - Benchmark of the Compute-in-Memory-Based DNN Accelerator With Area Constraint(2020)
* [ISCA](https://dblp.org/db/conf/isca/index.html)
  - ISAAC: a convolutional neural network accelerator with in-situ analog arithmetic in crossbars(2016)
  - PRIME: a novel processing-in-memory architecture for neural network computation in ReRAM-based main memory(2016)
  - In-Datacenter Performance Analysis of a Tensor Processing Unit(2017)
  - SCALEDEEP:A Scalable Compute Architecture for Learning and Evaluating Deep Networks(2017)
  - SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks(2017)
  - ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks(2021)
* [IEDM](https://ieeexplore.ieee.org/xpl/conhome/1000245/all-proceedings)
  - NeuroSim+: An integrated device-to-algorithm framework for benchmarking synaptic devices and array architectures(2017)
  - DNN+NeuroSim: An End-to-End Benchmarking Framework for Compute-in-Memory Accelerators with Versatile Device Technologies(2019)
* [ASPLOS](https://dblp.org/db/conf/asplos/index.html)
  - PUMA: A Programmable Ultra-efficient Memristor-based Accelerator for Machine Learning Inference(2019)
* [DAC](https://dblp.org/db/conf/dac/index.html)
  - PIMCOMP: A Universal Compilation Framework for Crossbar-based PIM DNN Accelerators(2023)
  - AutoDCIM: An Automated Digital CIM Compiler(2023)
  - A Convolution Neural Network Accelerator Design with Weight Mapping and Pipeline Optimization(2023)
* [HPCA](https://dblp.org/db/conf/hpca/index.html)
  - PipeLayer: A Pipelined ReRAM-Based Accelerator for Deep Learning(2017)
  - A3: Accelerating Attention Mechanisms in Neural Networks with Approximation(2020)
* [TCAS-I](https://dblp.org/db/journals/tcasI/index.html)
  - ENNA: An Efficient Neural Network Accelerator Design Based on ADC-Free Compute-In-Memory Subarrays(2023)
* [ICCAD](https://dblp.org/db/conf/iccad/index.html)
  - Scaling the "memory wall"(2012)
  - ReTransformer: ReRAM-based processing-in-memory architecture for transformer acceleration(2020)
  - GAMMA: automating the HW mapping of DNN models on accelerators via genetic algorithm(2020)
  - Design Space and Memory Technology Co-Exploration for In-Memory Computing Based Machine Learning Accelerators(2022)
* ASPDAC
  - Improving the Robustness and Efficiency of PIM-Based Architecture by SW/HW Co-Design(2023)
* [DATE](https://dblp.org/db/conf/date/index.html)
  - TDO-CIM: Transparent Detection and Offloading for Computation In-memory(2020)
  - A Runtime Reconfigurable Design of Compute-in-Memory based Hardware Accelerator(2021)
* [ISCAS](https://dblp.org/db/conf/iscas/index.html)
  - Optimizing Weight Mapping and Data Flow for Convolutional Neural Networks on RRAM Based Processing-In-Memory Architecture(2019)
* [TCAD](https://dblp.org/db/journals/tcad/index.html)
  - ESSENCE: Exploiting Structured Stochastic Gradient Pruning for Endurance-Aware ReRAM-Based In-Memory Training Systems(2023)
* [MWSCAS](https://dblp.org/db/conf/mwscas/index.html)
  - 8T XNOR-SRAM based Parallel Compute-in-Memory for Deep Neural Network Accelerator(2020)
* Others
  - DRAMSim2: A Cycle Accurate Memory System Simulator(2011)
  - Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks(2016)
  - Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1(2016)
  - Ramulator: A Fast and Extensible DRAM Simulator(2016)
  - Efficient Processing of Deep Neural Networks: A Tutorial and Survey(2017)
  - FINN: A Framework for Fast, Scalable Binarized Neural Network Inference(2017)
  - A Lightweight YOLOv2: A Binarized CNN with A Parallel Support Vector Regression for an FPGA(2018)
  - NeuroSim: A Circuit-Level Macro Model for Benchmarking Neuro-Inspired Architectures in Online Learning(2018)
  - Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices(2019)
  - Device and materials requirements for neuromorphic computing(2019)
  - Compute-in-RRAM with Limited On-chip Resources(2021)
  - Compute-in-Memory Chips for Deep Learning: Recent Trends and Prospects(2021)
  - NeuroSim Simulator for Compute-in-Memory Hardware Accelerator: Validation and Benchmark(2021)
  - Benchmarking Memory-Centric Computing Systems: Analysis of Real Processing-In-Memory Hardware(2021)
  - Digital Versus Analog Artificial Intelligence Accelerators: Advances, trends, and emerging designs(2022)
  - ULECGNet: An Ultra-Lightweight End-to-End ECG Classification Neural Network(2022)
  - Design Methodology and Trends of SRAM-Based Compute-in-Memory Circuits(2022)
  - From Macro To Microarchitecture: Reviews and Trends of SRAM-Based Compute-in-Memory Circuits(2023)
  - Side-Channel Attack Analysis on In-Memory Computing Architectures(2023)

## Machine Learning
  - Attention Is All You Need(2017)
  - Training and Inference with Integers in Deep Neural Networks(2018)
  - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2019)
  - Q8BERT: Quantized 8Bit BERT(2019)
  - Language Models are Few-Shot Learners(2020)
