---
layout: single
title: "Paper Reading"
permalink: /reading list/
author_profile: true
---

## AI Chip/Accelerator
* Nature
  - Neuro-inspired computing chips(2020)
* Science
  - Edge learning using a fully integrated neuro-inspired memristor chip(2023)
* ISSCC
  - A 1.42TOPS/W Deep Convolutional Neural Network Recognition Processor for Intelligent IoE Systems(2016)
  - A 288μW Programmable Deep-Learning Processor with 270KB On-Chip Weight Storage Using Non-Uniform Memory Hierarchy for Mobile Intelligence(2017)
  - A 0.62mW Ultra-Low-Power Convolutional-NeuralNetwork Face-Recognition Processor and a CIS Integrated with Always-On Haar-Like Face Detector(2017)
  - A 2.9TOPS/W Deep Convolutional Neural Network SoC in FD-SOI 28nm for Intelligent Embedded Systems(2017)
  - DNPU: An 8.1TOPS/W Reconfigurable CNN-RNN Processor for General-Purpose Deep Neural Networks(2017)
  - UNPU: A 50.6TOPS/W Unified Deep Neural Network Accelerator with 1b-to-16b Fully-Variable Weight Bit-Precision(2018)
  - A 65nm 4Kb Algorithm-Dependent Computing-inMemory SRAM Unit-Macro with 2.3ns and 55.8TOPS/W Fully Parallel Product-Sum Operation for Binary DNN Edge Processors(2018)
  - A 28nm 64Kb 6T SRAM Computing-in-Memory Macro with 8b MAC Operation for AI Edge Chips(2020)
  - An 89TOPS/W and 16.3TOPS/mm2 All-Digital SRAM-Based Full-Precision Compute-In-Memory Macro in 22nm for Machine-Learning Edge Applications(2021)
* JSSC
  - Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks(2017)
  - A High Energy Efficient Reconfigurable Hybrid Neural Network Processor for Deep Learning Applications(2018)
  - Evolver: A Deep Learning Processor With On-Device Quantization–Voltage–Frequency Tuning(2021)
  - TranCIM: Full-Digital Bitline-Transpose CIM-based Sparse Transformer Accelerator With Pipeline/Parallel Reconfigurable Modes(2023)
  - ReDCIM: Reconfigurable Digital ComputingIn-Memory Processor With Unified FP/INT Pipeline for Cloud AI Acceleration(2023)
* VLSI
  - Deep Convolutional Neural Network Architecture With Reconfigurable Computation Patterns(2017)
  - Benchmark of the Compute-in-Memory-Based DNN Accelerator With Area Constraint(2020)
  - A 40nm Analog-Input ADC-Free Compute-in-Memory RRAM Macro with Pulse-Width Modulation between Sub-arrays(2022)
* ISCA
  - In-Datacenter Performance Analysis of a Tensor Processing Unit(2017)
  - SCALEDEEP:A Scalable Compute Architecture for Learning and Evaluating Deep Networks(2017)
  - SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks(2017)
  - ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks(2021)
* IEDM
  - NeuroSim+: An integrated device-to-algorithm framework for benchmarking synaptic devices and array architectures(2017)
  - DNN+NeuroSim: An End-to-End Benchmarking Framework for Compute-in-Memory Accelerators with Versatile Device Technologies(2019)
* ASPLOS
  - PUMA: A Programmable Ultra-efficient Memristor-based Accelerator for Machine Learning Inference(2019)
* DAC
  - PIMCOMP: A Universal Compilation Framework for Crossbar-based PIM DNN Accelerators(2023)
  - AutoDCIM: An Automated Digital CIM Compiler(2023)
* HPCA
  - A3: Accelerating Attention Mechanisms in Neural Networks with Approximation(2020)
* TCAS-I
  - ENNA: An Efficient Neural Network Accelerator Design Based on ADC-Free Compute-In-Memory Subarrays(2023)
* ICCAD
  - Scaling the "memory wall"(2012)
  - ReTransformer: ReRAM-based processing-in-memory architecture for transformer acceleration(2020)
* DATE
  - A Runtime Reconfigurable Design of Compute-in-Memory based Hardware Accelerator(2021)
* ISCAS
  - Optimizing Weight Mapping and Data Flow for Convolutional Neural Networks on RRAM Based Processing-In-Memory Architecture(2019)
* TCAD
  - ESSENCE: Exploiting Structured Stochastic Gradient Pruning for Endurance-Aware ReRAM-Based In-Memory Training Systems(2023)
* MWSCAS
  - 8T XNOR-SRAM based Parallel Compute-in-Memory for Deep Neural Network Accelerator(2020)
* Others
  - DRAMSim2: A Cycle Accurate Memory System Simulator(2011)
  - Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks(2016)
  - Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1(2016)
  - Ramulator: A Fast and Extensible DRAM Simulator(2016)
  - Efficient Processing of Deep Neural Networks: A Tutorial and Survey(2017)
  - FINN: A Framework for Fast, Scalable Binarized Neural Network Inference(2017)
  - A Lightweight YOLOv2: A Binarized CNN with A Parallel Support Vector Regression for an FPGA(2018)
  - NeuroSim: A Circuit-Level Macro Model for Benchmarking Neuro-Inspired Architectures in Online Learning(2018)
  - Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices(2019)
  - Device and materials requirements for neuromorphic computing(2019)
  - Compute-in-RRAM with Limited On-chip Resources(2021)
  - Compute-in-Memory Chips for Deep Learning: Recent Trends and Prospects(2021)
  - NeuroSim Simulator for Compute-in-Memory Hardware Accelerator: Validation and Benchmark(2021)
  - Benchmarking Memory-Centric Computing Systems: Analysis of Real Processing-In-Memory Hardware(2021)
  - Digital Versus Analog Artificial Intelligence Accelerators: Advances, trends, and emerging designs(2022)
  - ULECGNet: An Ultra-Lightweight End-to-End ECG Classification Neural Network(2022)
  - Design Methodology and Trends of SRAM-Based Compute-in-Memory Circuits(2022)
  - From Macro To Microarchitecture: Reviews and Trends of SRAM-Based Compute-in-Memory Circuits(2023)
  - Side-Channel Attack Analysis on In-Memory Computing Architectures(2023)

## Machine Learning
  - Attention Is All You Need(2017)
  - Training and Inference with Integers in Deep Neural Networks(2018)
  - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2019)
  - Q8BERT: Quantized 8Bit BERT(2019)
  - Language Models are Few-Shot Learners(2020)
