---
layout: single
title: "Paper Reading"
permalink: /reading list/
author_profile: false
---

## AI Chip/Accelerator
* [Nature](https://www.nature.com/search?journal=ncomms,%20nature,%20natelectron&article_type=research&subject=engineering,%20nanoscience-and-technology&order=relevance)
  - Neuro-inspired computing chips(2020)
* [Science](https://www.science.org/action/doSearch?AllField=Edge+learning+using+a+fully+integrated+neuro-inspired+memristor+chip)
  - Edge learning using a fully integrated neuro-inspired memristor chip(2023)
* [ISSCC](https://dblp.org/db/conf/isscc/index.html)
  - A 1.42TOPS/W Deep Convolutional Neural Network Recognition Processor for Intelligent IoE Systems(2016)
  - A 288μW Programmable Deep-Learning Processor with 270KB On-Chip Weight Storage Using Non-Uniform Memory Hierarchy for Mobile Intelligence(2017)
  - A 0.62mW Ultra-Low-Power Convolutional-NeuralNetwork Face-Recognition Processor and a CIS Integrated with Always-On Haar-Like Face Detector(2017)
  - A 2.9TOPS/W Deep Convolutional Neural Network SoC in FD-SOI 28nm for Intelligent Embedded Systems(2017)
  - DNPU: An 8.1TOPS/W Reconfigurable CNN-RNN Processor for General-Purpose Deep Neural Networks(2017)
  - UNPU: A 50.6TOPS/W Unified Deep Neural Network Accelerator with 1b-to-16b Fully-Variable Weight Bit-Precision(2018)
  - A 65nm 4Kb Algorithm-Dependent Computing-inMemory SRAM Unit-Macro with 2.3ns and 55.8TOPS/W Fully Parallel Product-Sum Operation for Binary DNN Edge Processors(2018)
  - A 28nm 64Kb 6T SRAM Computing-in-Memory Macro with 8b MAC Operation for AI Edge Chips(2020)
  - A Programmable Neural-Network Inference Accelerator Based on Scalable In-Memory Computing(2021)
  - An 89TOPS/W and 16.3TOPS/mm2 All-Digital SRAM-Based Full-Precision Compute-In-Memory Macro in 22nm for Machine-Learning Edge Applications(2021)
* [JSSC](https://dblp.org/db/journals/jssc/index.html)
  - Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks(2017)
  - A High Energy Efficient Reconfigurable Hybrid Neural Network Processor for Deep Learning Applications(2018)
  - Evolver: A Deep Learning Processor With On-Device Quantization–Voltage–Frequency Tuning(2021)
  - TranCIM: Full-Digital Bitline-Transpose CIM-based Sparse Transformer Accelerator With Pipeline/Parallel Reconfigurable Modes(2023)
  - ReDCIM: Reconfigurable Digital ComputingIn-Memory Processor With Unified FP/INT Pipeline for Cloud AI Acceleration(2023)
* [IEDM](https://ieeexplore.ieee.org/xpl/conhome/1000245/all-proceedings)
  - NeuroSim+: An integrated device-to-algorithm framework for benchmarking synaptic devices and array architectures(2017)
  - DNN+NeuroSim: An End-to-End Benchmarking Framework for Compute-in-Memory Accelerators with Versatile Device Technologies(2019)
* [VLSI](https://dblp.org/db/conf/vlsic/index.html)
  - A 1.06-to-5.09 TOPS/W reconfigurable hybrid-neural-network processor for deep learning applications(2017)
  - A 40nm Analog-Input ADC-Free Compute-in-Memory RRAM Macro with Pulse-Width Modulation between Sub-arrays(2022)
* [ISCA](https://dblp.org/db/conf/isca/index.html)
  - ISAAC: a convolutional neural network accelerator with in-situ analog arithmetic in crossbars(2016)
  - PRIME: a novel processing-in-memory architecture for neural network computation in ReRAM-based main memory(2016)
  - In-Datacenter Performance Analysis of a Tensor Processing Unit(2017)
  - SCALEDEEP:A Scalable Compute Architecture for Learning and Evaluating Deep Networks(2017)
  - SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks(2017)
  - ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks(2021)
* [ASPLOS](https://dblp.org/db/conf/asplos/index.html)
  - DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning(2014)
  - PUMA: A Programmable Ultra-efficient Memristor-based Accelerator for Machine Learning Inference(2019)
* [DAC](https://dblp.org/db/conf/dac/index.html)
  - Atomlayer: a universal ReRAM-based CNN accelerator with atomic layer computation(2018)
  - A Configurable Multi-Precision CNN Computing Framework Based on Single Bit RRAM(2019)
  - A Two-way SRAM Array based Accelerator for Deep Neural Network On-chip Training(2020)
  - HERO: hessian-enhanced robust optimization for unifying and improving generalization and quantization performance(2022)
  - PIMCOMP: A Universal Compilation Framework for Crossbar-based PIM DNN Accelerators(2023)
  - AutoDCIM: An Automated Digital CIM Compiler(2023)
  - A Convolution Neural Network Accelerator Design with Weight Mapping and Pipeline Optimization(2023)
  - PIM-HLS: An Automatic Hardware Generation Tool for Heterogeneous Processing-In-Memory-based Neural Network Accelerators(2023)
* [HPCA](https://dblp.org/db/conf/hpca/index.html)
  - PipeLayer: A Pipelined ReRAM-Based Accelerator for Deep Learning(2017)
  - A3: Accelerating Attention Mechanisms in Neural Networks with Approximation(2020)
  - MAGMA: An Optimization Framework for Mapping Multiple DNNs on Multiple Accelerator Cores(2022)
* [TC](https://dblp.org/db/journals/tc/index.html)
  - CIMAT: A Compute-In-Memory Architecture for On-chip Training Based on Transpose SRAM Arrays(2020)
  - Device-Circuit-Architecture Co-Exploration for Computing-in-Memory Neural Accelerators(2021)
* [TCAS-I](https://dblp.org/db/journals/tcasI/index.html)
  - Research Progress on Memristor: From Synapses to Computing Systems(2022)
  - ENNA: An Efficient Neural Network Accelerator Design Based on ADC-Free Compute-In-Memory Subarrays(2023)
* [TCAD](https://dblp.org/db/journals/tcad/index.html)
  - MNSIM: Simulation Platform for Memristor-Based Neuromorphic Computing System(2018)
  - DNN+NeuroSim V2.0: An End-to-End Benchmarking Framework for Compute-in-Memory Accelerators for On-Chip Training(2021)
  - OCC: An Automated End-to-End Machine Learning Optimizing Compiler for Computing-In-Memory(2022)
  - ESSENCE: Exploiting Structured Stochastic Gradient Pruning for Endurance-Aware ReRAM-Based In-Memory Training Systems(2023)
  - A Coordinated Model Pruning and Mapping Framework for RRAM-Based DNN Accelerators(2023)
* [TVLSI](https://dblp.org/db/journals/tvlsi/index.html)
  - Deep Convolutional Neural Network Architecture With Reconfigurable Computation Patterns(2017)
  - Benchmark of the Compute-in-Memory-Based DNN Accelerator With Area Constraint(2020)
* [ICCAD](https://dblp.org/db/conf/iccad/index.html)
  - Scaling the "memory wall"(2012)
  - OpenRAM: An open-source memory compiler(2016)
  - Accelergy: An Architecture-Level Energy Estimation Methodology for Accelerator Designs(2019)
  - ReTransformer: ReRAM-based processing-in-memory architecture for transformer acceleration(2020)
  - GAMMA: automating the HW mapping of DNN models on accelerators via genetic algorithm(2020)
  - Multi-Objective Optimization of ReRAM Crossbars for Robust DNN Inferencing under Stochastic Noise(2021)
  - Design Space and Memory Technology Co-Exploration for In-Memory Computing Based Machine Learning Accelerators(2022)
* [DATE](https://dblp.org/db/conf/date/index.html)
  - ReCom: An efficient resistive accelerator for compressed deep neural networks(2018)
  - TDO-CIM: Transparent Detection and Offloading for Computation In-memory(2020)
  - A Fast and Energy Efficient Computing-in-Memory Architecture for Few-Shot Learning Applications(2020)
  - A Runtime Reconfigurable Design of Compute-in-Memory based Hardware Accelerator(2021)
  - Gibbon: Efficient Co-Exploration of NN Model and Processing-In-Memory Architecture(2022)
* [ASPDAC](https://dblp.org/db/conf/aspdac/index.html)
  - ReGAN: A pipelined ReRAM-based accelerator for generative adversarial networks(2018)
  - Learning the sparsity for ReRAM: mapping and pruning sparse neural network for ReRAM based accelerator(2019)
  - Improving the Robustness and Efficiency of PIM-Based Architecture by SW/HW Co-Design(2023)
* [ISCAS](https://dblp.org/db/conf/iscas/index.html)
  - Optimizing Weight Mapping and Data Flow for Convolutional Neural Networks on RRAM Based Processing-In-Memory Architecture(2019)
  - MINT: Mixed-Precision RRAM-Based IN-Memory Training Architecture(2020)
* [MWSCAS](https://dblp.org/db/conf/mwscas/index.html)
  - 8T XNOR-SRAM based Parallel Compute-in-Memory for Deep Neural Network Accelerator(2020)
* Others
  - DRAMSim2: A Cycle Accurate Memory System Simulator(2011)
  - Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks(2016)
  - Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1(2016)
  - Ramulator: A Fast and Extensible DRAM Simulator(2016)
  - Efficient Processing of Deep Neural Networks: A Tutorial and Survey(2017)
  - FINN: A Framework for Fast, Scalable Binarized Neural Network Inference(2017)
  - A Lightweight YOLOv2: A Binarized CNN with A Parallel Support Vector Regression for an FPGA(2018)
  - NeuroSim: A Circuit-Level Macro Model for Benchmarking Neuro-Inspired Architectures in Online Learning(2018)
  - Motivation for and Evaluation of the First Tensor Processing Unit(2018)
  - mRNA: Enabling Efficient Mapping Space Exploration for a Reconfiguration Neural Accelerator(2019)
  - In-Memory Computing: Advances and Prospects(2019)
  - Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices(2019)
  - Device and materials requirements for neuromorphic computing(2019)
  - Timeloop: A Systematic Approach to DNN Accelerator Evaluation(2019)
  - Exploring Bit-Slice Sparsity in Deep Neural Networks for Efficient ReRAM-Based Deployment(2019)
  - MNSIM 2.0: A Behavior-Level Modeling Tool for Memristor-based Neuromorphic Computing Systems(2020)
  - An Architecture-Level Energy and Area Estimator for Processing-In-Memory Accelerator Designs(2020)
  - Compute-in-RRAM with Limited On-chip Resources(2021)
  - Compute-in-Memory Chips for Deep Learning: Recent Trends and Prospects(2021)
  - NeuroSim Simulator for Compute-in-Memory Hardware Accelerator: Validation and Benchmark(2021)
  - Benchmarking Memory-Centric Computing Systems: Analysis of Real Processing-In-Memory Hardware(2021)
  - Evaluating Spatial Accelerator Architectures with Tiled Matrix-Matrix Multiplication(2022)
  - Digital Versus Analog Artificial Intelligence Accelerators: Advances, trends, and emerging designs(2022)
  - ULECGNet: An Ultra-Lightweight End-to-End ECG Classification Neural Network(2022)
  - Design Methodology and Trends of SRAM-Based Compute-in-Memory Circuits(2022)
  - Tolerating Noise Effects in Processing-in-Memory Systems for Neural Networks: A Hardware–Software Codesign Perspective(2022)
  - On Building Efficient and Robust Neural Network Designs(2022)
  - ReaLPrune: ReRAM Crossbar-aware Lottery Ticket Pruned CNNs(2022)
  - From Macro To Microarchitecture: Reviews and Trends of SRAM-Based Compute-in-Memory Circuits(2023)
  - Side-Channel Attack Analysis on In-Memory Computing Architectures(2023)
  - An Ultra-Low Power TinyML System for Real-Time Visual Processing at Edge(2023)
  - Hardware-aware Quantization/Mapping Strategies for Compute-in-Memory Accelerators(2023)
  - Wafer-scale Computing: Advancements, Challenges, and Future Perspectives(2023)
  - Neuro-Symbolic Computing: Advancements and Challenges in Hardware-Software Co-Design(2023)
  - Block-Wise Mixed-Precision Quantization: Enabling High Efficiency for Practical ReRAM-based DNN Accelerators(2023)

## Machine Learning
  - Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures(2009)
  - Training deep neural networks with low precision multiplications(2015)
  - Attention Is All You Need(2017)
  - Training and Inference with Integers in Deep Neural Networks(2018)
  - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2019)
  - Q8BERT: Quantized 8Bit BERT(2019)
  - Machine Learning at Facebook: Understanding Inference at the Edge(2019)
  - Language Models are Few-Shot Learners(2020)
  - Training high-performance and large-scale deep neural networks with full 8-bit integers(2020)
  - Low Latency Deep Learning Inference Model for Distributed Intelligent IoT Edge Clusters(2021)

## RISC-V
  - A 45nm 1.3GHz 16.7 double-precision GFLOPS/W RISC-V processor with vector accelerators(2014)
  - TAIGA: A new RISC-V soft-processor framework enabling high performance CPU architectural features(2017)
  - Framework and Tools for Undergraduates Designing RISC-V Processors on an FPGA in Computer Architecture Education(2019)
  - Open-Source RISC-V Processor IP Cores for FPGAs — Overview and Evaluation(2019)
  - GVSoC: A Highly Configurable, Fast and Accurate Full-Platform Simulator for RISC-V based IoT Processors(2021)
  - RVfpga: Using a RISC-V Core Targeted to an FPGA in Computer Architecture Education(2021)
  - Design and verification of RISC-V CPU based on HLS and UVM(2021)
  - A comparative survey of open-source application-class RISC-V processor implementations(2021)
  - A review of CNN accelerators for embedded systems based on RISC-V(2022)
  - A Survey of RISC-V CPU for IoT Applications(2022)
